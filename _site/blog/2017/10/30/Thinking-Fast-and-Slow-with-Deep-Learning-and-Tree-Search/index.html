<!DOCTYPE html>
<html lang="en-us">
<!--  
====================================================
Homepage: http://localhost:4000
Credits: http://localhost:4000/disclosure
====================================================
-->
<head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<!-- link href="http://gmpg.org/xfn/11" rel="profile" -->
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8">

<!-- Enable responsiveness on mobile devices-->
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

<title>
  
    Thinking Fast and Slow with Deep Learning and Tree Search &middot; David Barber
  
</title>

<!-- Search Engine Optimization -->
<meta name="description" content="Reinforcement Learning">
<meta name="keywords" content="deep learning, Monte Carlo Tree Search, Hex, reinforcement learning, AlphaGo, Dual Process Theory">




<!-- Twitter Cards -->
  
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@davidobarber">
<meta name="twitter:image" content="http://web4.cs.ucl.ac.uk/staff/D.Barber/images/sitelogo.png">


<meta name="twitter:title" content="Thinking Fast and Slow with Deep Learning and Tree Search">
<meta name="twitter:description" content="Reinforcement Learning">
<meta name="twitter:creator" content="@davidobarber">
<!-- End Twitter Cards -->

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Thinking Fast and Slow with Deep Learning and Tree Search">
<meta property="og:description" content="Reinforcement Learning">
<meta property="og:url" content="http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/">
<meta property="og:site_name" content="David Barber">

<meta property="og:image" content="http://web4.cs.ucl.ac.uk/staff/D.Barber/images/sitelogo.png">

<meta property="fb:app_id" content="1003108156422006">
<meta property="fb:admins" content="817465054">

<!-- Fonts -->
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400|Tangerine|Inconsolata">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/public/css/iconmoon.css">

<!-- CSS -->
<link rel="stylesheet" href="/public/css/style.min.css">

<!-- Add-on CSS to override system-wide defaults -->
<link rel="stylesheet" href="/public/css/addon.css">

<!-- CSS override per page -->


<!-- Java scripts -->
<!-- <script src="/public/js/jquery.min.js"></script> -->

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/icons/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/icons/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/icons/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/icons/apple-touch-icon-144x144-precomposed.png">
<!-- 180x180 (precomposed) for iPhone 6 -->
<link rel="apple-touch-icon-precomposed" sizes="180x180" href="http://localhost:4000/images/icons/apple-touch-icon-180x180.png">
<!-- 192x192 (precomposed) for Android -->
<link rel="icon" type="image/png" sizes="192x192"  href="http://localhost:4000/images/icons/android-icon-192x192.png">


<link rel="canonical" href="http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/">


<!-- RSS -->
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/feed.xml">


  <!--Load Mathjax-->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'" 
                }
            },
            showProcessingMessages: false
        });
    </script>

<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ['\(', '\)'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->



</head>


<!--<body class="theme-base-08">-->
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<!--<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">-->
<!--e<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" checked>-->

<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" >

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
<!--   <div class="sidebar-item">
    <p>David's blog...</p>
  </div> -->

  <nav class="sidebar-nav">
    <!-- a class="sidebar-nav-item" href="/">Home</a-->

    

    

    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/"><i class="iconside iconm-home"></i> Home</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/about/"><i class="iconside iconm-user"></i> About</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/blog/"><i class="iconside iconm-quill"></i> Blog</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/tags/"><i class="fa fa-tags"></i> Tags</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/archive/"><i class="fa fa-archive"></i> Archive</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/contact/"><i class="iconside iconm-envelop"></i> Contact</a>
                
    
            
                    <a class="sidebar-nav-item" href="https://twitter.com/davidobarber" target="_blank"><i class="iconside iconm-twitter"></i> Twitter</a>
                
    
            
                    <a class="sidebar-nav-item" href="http://localhost:4000/feed.xml"><i class="iconside iconm-feed2"></i> Feed</a>
                
         

    <a class="sidebar-nav-item" href=" http://www.cs.ucl.ac.uk/staff/d.barber/">UCL page</a>

  </nav>

<hr class="gh">




</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">David Barber</a>
            <!-- <small></small> -->
            <div class="headicons">
              <small><a href="http://localhost:4000/about" rel="me" title="About"><i class="iconm iconm-user"></i></a></small>
              <small><a href="http://localhost:4000/blog" rel="me" title="Blog"><i class="iconm iconm-quill"></i></a></small>
              <small><a href="http://localhost:4000/contact" rel="me" title="Contact"><i class="iconm iconm-envelop"></i></a></small> 
            </div>           
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 itemprop="name" class="post-title">Thinking Fast and Slow with Deep Learning and Tree Search</h1>
  <span class="post-date" itemprop="datePublished" content="2017-10-30"><i class="fa fa-calendar"
  title="Date published"> <a class="permalink"
  href="http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/" itemprop="url" title="Permanent link to this post">30 Oct 2017</a> </i></span>
  
  <span class="post-tags" itemprop="keywords" content="deep learning, Monte Carlo Tree Search, Hex, reinforcement learning, AlphaGo, and Dual Process Theory"><i class="fa fa-tags" title="page tags"></i> <a href="http://localhost:4000/tags/#deep+learning" title="Pages tagged deep learning" rel="tag">deep learning</a> &bull;  <a href="http://localhost:4000/tags/#Monte+Carlo+Tree+Search" title="Pages tagged Monte Carlo Tree Search" rel="tag">Monte Carlo Tree Search</a> &bull;  <a href="http://localhost:4000/tags/#Hex" title="Pages tagged Hex" rel="tag">Hex</a> &bull;  <a href="http://localhost:4000/tags/#reinforcement+learning" title="Pages tagged reinforcement learning" rel="tag">reinforcement learning</a> &bull;  <a href="http://localhost:4000/tags/#AlphaGo" title="Pages tagged AlphaGo" rel="tag">AlphaGo</a> &bull;  <a href="http://localhost:4000/tags/#Dual+Process+Theory" title="Pages tagged Dual Process Theory" rel="tag">Dual Process Theory</a></span>
    
      <span class="social-icons"><a href="https://www.facebook.com/sharer.php?u=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/"  class="social-icons" target="_blank" title="Share on facebook"> <i class="fa fa-facebook meta"></i></a> 
<a href="https://twitter.com/share?text=Thinking Fast and Slow with Deep Learning and Tree Search&amp;url=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/&amp;via=davidobarber"  class="social-icons" target="_blank" title="Share on twitter"> <i class="fa fa-twitter meta"></i></a>
<a href="https://plus.google.com/share?url=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/"  class="social-icons" target="_blank" title="Share on Google+"> <i class="fa fa-google-plus"></i></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/"  class="social-icons" target="_blank" title="Share on LinkedIn"> <i class="fa fa-linkedin"></i></a>
<a href="http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/#disqus_thread" class="social-icons" title="Comments"><i class="fa fa-comments"></i></a>
<!--<a href="http://localhost:4000/featured" title="Featured posts"><i class="fa fa-paperclip" title="Featured" class="social-icons" style="vertical-align: top;"></i></a>-->
<a href="javascript:window.print()" class="social-icons" title="Printer friendly format"><i class="fa fa-print"></i></a>
</span>

    
  <p>How to train powerful reinforcement learning agents by Thinking Fast and Slow.</p>

<!--more-->


<p><script type="math/tex">\newcommand{\sq}[1]{\left[#1\right]}</script>
<script type="math/tex">\newcommand{\ave}[1]{\mathbb{E}\sq{#1}}</script></p>

<h2 class="no_toc" id="dual-process-theory">Dual Process Theory</h2>

<p>According to <a href="https://en.wikipedia.org/wiki/Dual_process_theory">dual-process theory</a> human reasoning consists of two different kinds of thinking.
System 1, is a fast, unconscious and automatic mode of thought, also known as intuition or heuristic process. System 2, an evolutionarily recent process unique to humans, is a slow, conscious, explicit and rule-based mode of reasoning.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//behaviour-design-predicting-irrational-decisions-12-638.jpg" alt="dual process" title="dual process theory" />
<a href="https://www.slideshare.net/AshDonaldson/behaviour-design-predicting-irrational-decisions">image credit</a></p>

<p>When learning to complete a challenging planning task, such as playing a board game, humans exploit both processes: strong intuitions allow for more effective analytic reasoning by rapidly selecting interesting lines of play for consideration. Repeated deep study gradually improves intuitions. Stronger intuitions feedback to stronger analysis, creating a closed learning loop. In other words, humans learn by thinking fast and slow.</p>

<!--### What's wrong with Deep RL?
{:.no_toc}-->

<p>In current Deep Reinforcement Learning algorithms such as Policy Gradients<sup id="fnref:Williams"><a href="#fn:Williams" class="footnote">1</a></sup> and DQN<sup id="fnref:DQN"><a href="#fn:DQN" class="footnote">2</a></sup>, neural networks make action selections with no lookahead; this is analogous to System 1. Unlike human intuition, their training does not benefit from a ‘System 2’ to suggest strong policies. Another issue is that state-of-the-art RL board playing algorithms require an initial database of human expert play<sup id="fnref:AlphaGo"><a href="#fn:AlphaGo" class="footnote">3</a></sup>. Making a state-of-the-art board game player <em>ex nihilo</em> is a major challenge for AI.</p>

<h2 class="no_toc" id="expert-iteration-exit">Expert Iteration (ExIt)</h2>

<p>In our <a href="">NIPSexitpaper</a>, we introduced Expert Iteration (ExIt), which is a new and general framework for learning.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//ExIt.png" alt="ExIt" title="Expert Iteration" /></p>

<p>ExIt can be viewed as an extension of Imitation Learning (IL) methods to domains where the best known experts are unable to achieve satisfactory performance. In stanadard IL an apprentice is trained to imitate the behaviour of an expert.  In ExIt, we extend this to an iterative learning process.  Between each iteration, we perform an Expert Improvement step, where we bootstrap the (fast) apprentice policy to increase the performance of the (comparatively slow) expert.</p>

<p>To give some intuition around this idea, consider playing a board game such as chess. Here the expert is analogous to a strong (but slow) chess player, and the apprentice is analogous to an initially novice (but quick thinking) chess player.</p>

<p>Initially the expert player plays some games against an opponent. The expert is a strong player, thinking deep (and slow) about each move. The apprentice observes the chess board state and each eventual move made by the expert and then tries his best to learn to quickly imitate the move made by the expert in each of the observed board positions . In an algorithm, this imitation could be done, for example, by fitting a neural network to the move made by the expert from a game position.  The apprentice learns a fast policy that is able to quickly imitate the play of the expert on the moves seen so far.  A key point here is that the apprentice learns in such a way that they are able to generalise and then apply their quick intution on positions not previously seen – the neural network thus plays the role of both generalising and imitating the play of the expert.</p>

<p>Now that the apprentice has learned a fast imitation of the expert (on the moves seen so far), she can try to be of use to the expert.  One approach would be that when the expert now wishes to make a move, a small set of candidate moves are suggested very quickly by the apprentice which the expert can then consider in depth, possibly also guided during this slow thought process by other quick suggestions by the apprentice.</p>

<p>At the end of this phase, the expert will have made a set of apprentice-aided moves, with each move being typically  much stronger than the apprentice could have made with the deep (but slow) thought of the expert.</p>

<p>The above process now repeats, with the apprentice retraining on the moves suggested by the expert. This completes one full iteration of the learning phase of the apprentice and we iterate this process until we either run out of time or the apprentice learns to perform at roughly the same level as the expert.</p>

<p>From a Dual Process perspective, the Imitation Learning step is analogous to a human improving their intuition for the task by studying example problems, while the Expert Improvement step is analogous to a human using the their improved intuition to guide future analysis</p>

<h3 class="no_toc" id="tree-search-and-deep-learning">Tree Search and Deep Learning</h3>

<p>Exit is a very general strategy for learning and the apprentice and expert can be specified in a variety of ways. In board games Monte Carlo Tree Search is a strong playing strategy<sup id="fnref:MCTS"><a href="#fn:MCTS" class="footnote">4</a></sup> and is a natural candidate to play the role of the expert.  Deep Learning has been shown to be a successfull method to imitate the play of strong players and it is therefore a natural candidate to play the role of the apprentice<sup id="fnref:AlphaGo:1"><a href="#fn:AlphaGo" class="footnote">3</a></sup>.</p>

<p>The expert policy is calculated using a tree search algorithm. We use the apprentice to improve such an expert by using the apprentice policy to direct search effort towards promising moves, or by evaluating states encountered during search more quickly and accurately using our apprentice, effectively reducing the search breadth and depth. In other words, we bootstrap the knowledge acquired by Imitation Learning back into the planning algorithm.</p>

<!--The role of the expert is to perform exploration, and thereby to accurately determine strong move sequences, from a single position. The role of the apprentice is to generalise the policies that the expert discovers across the whole state space, and to provide rapid access to that strong policy for bootstrapping in future searches. The canonical choice of expert is a tree search algorithm. Search considers the exact dynamics of the game tree local to the state under consideration, and can be considered analogous to the lookahead human games players engage in when planning their moves. The bootstrap policy can be used to bias search towards promising moves, aid node evaluation, or both. By employing search, we can find promising sequences potentially far away from the bootstrap policy, accelerating learning in complex scenarios. Possible tree search algorithms include Monte Carlo Tree Search[^MCTS]-->

<h3 class="no_toc" id="the-boardgame-hex">The Boardgame Hex</h3>

<p><a href="https://en.wikipedia.org/wiki/Hex_(board_game)">Hex</a> is a classic two-player boardgame played on an <script type="math/tex">n\times n</script> hexagonal grid. The players, denoted by colours black and white, alternate placing stones of their colour in empty cells. The black player wins if there is a sequence of adjacent black stones connecting the North edge of the board to the South edge. White wins if he achieves a sequence of adjacent white stones running from the West edge to the East edge.</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//hexBW.png" alt="hex" title="Hex" /></p>

<p>The above represents play on a <script type="math/tex">5\times 5</script> board, with white winning (reproduced from [^Hex]).   Hex has deep strategy, making it challenging for machines to play and its large action set and connection-based rules means it shares similar challenges for AI to Go. Compared to Go, however, the rules are simpler and there can be no draws, making it an ideal testbed for AI.</p>

<p>Because the rules of Hex are so simple, the game is amenable to mathematical analysis and the current best machine player MOHEX<sup id="fnref:MOHEX"><a href="#fn:MOHEX" class="footnote">5</a></sup> uses a combination of Monte Carlo Tree Search and smart mathematical insights.  MOHEX has won every Computer Games Olympiad Hex tournament since 2009. MOHEX is also trained on datasets of human expert play.</p>

<p>We wanted to see if we can outperform MOHEX by using our ExIt strategy, learning trained tabula rasa, without game-specific knowledge or human example play, beside the rules of the game. To do this, our expert is a MCTS player that is guided by the apprentice neural network.  Our neural network is based on a deep convolutional network, as shown below</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//NN.png" alt="NN" title="NN" /></p>

<p>The expert improvement step is then modelled by using the modified MCTS formula</p>

<script type="math/tex; mode=display">UCT(s,a) + w \frac{\hat{\pi}(a|s)}{n(s,a)+1}</script>

<p>This formula is closely related to one found in Gelly and Silver<sup id="fnref:OnlineOffline"><a href="#fn:OnlineOffline" class="footnote">6</a></sup>. Here <script type="math/tex">s</script> is the state of the Hex board, <script type="math/tex">a</script> is a possible action (ie move) from <script type="math/tex">s</script>. The term <script type="math/tex">UCT(s,a)</script> represents the classical Upper Confidence Tree formula<sup id="fnref:MCTS:1"><a href="#fn:MCTS" class="footnote">4</a></sup> used in MCTS.</p>

<p>The additional term denotes apprentice  helping guide the search to more promising moves, with <script type="math/tex">\hat{\pi}</script> being the suggestion of the apprentice, and <script type="math/tex">n(s,a)</script> the number of visits currently made by the search algorithm through state <script type="math/tex">s</script> and taking action <script type="math/tex">a</script>; <script type="math/tex">w</script> is an empirically chosen weighting factor that balances the slow thinking of the expert with the fast intuition of the apprentice.</p>

<p>To generate the data for traiing the apprentice (during each Imitation Learning phase), the batch approach generates data afresh, discarding all data from previous iterations. In the online version we consider instead a buffer of the most recent moves generted and we also consider an exponentially weighted version that favours more recent moves generated (since the more recent moves will be be from a stronger player).  A comparison of these different approaches is given below in which we compare the strength of the learned algorithm against a measure of training time.  We also show a more classical approach, known as REINFORCE (see <a href="">NIPSexitpaper</a> for details)</p>

<p class="text-center"><img src="http://web4.cs.ucl.ac.uk/staff/D.Barber/images//BatchOnline.png" alt="results" title="results" /></p>

<p>What the figure shows is that the ExIt approach is considerably more effective than classical approaches.  Indeed, after training, our apprentice-aided MCTS player outperforms the best known machine Hex player, namely MOHEX, beating it in 71% of games played on a <script type="math/tex">9\times 9</script> board.</p>

<h3 class="no_toc" id="relation-to-alphago-zero">Relation to AlphaGo Zero</h3>

<p>AlphaGo Zero [? ], published shortly after this work was first published, also implements the ExIt algorithm, and shows that it is able to achieve state-of-the-art performance in Go. Blah…..</p>

<h2 class="no_toc" id="summary">Summary</h2>

<p>ExIt is great…… Blah</p>

<h3 class="no_toc" id="references">References</h3>

<div class="footnotes">
  <ol>
    <li id="fn:Williams">
      <p>R. J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Mach. Learn., 8(3-4):229–256, 1992.&nbsp;<a href="#fnref:Williams" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:DQN">
      <p>V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-Level Control through Deep Reinforcement Learning. Nature, 518(7540):529–533, 2015.&nbsp;<a href="#fnref:DQN" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:AlphaGo">
      <p>D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature, 529(7587):484–489, 2016.&nbsp;<a href="#fnref:AlphaGo" class="reversefootnote">&#8617;</a>&nbsp;<a href="#fnref:AlphaGo:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:MCTS">
      <p>L. Kocsis and C. Szepesvári. Bandit Based Monte-Carlo Planning. In European Conference on Machine Learning, pages 282–293. Springer, 2006.&nbsp;<a href="#fnref:MCTS" class="reversefootnote">&#8617;</a>&nbsp;<a href="#fnref:MCTS:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:MOHEX">
      <p>S.-C. Huang, B. Arneson, R. Hayward, M. Müller, and J. Pawlewicz. MoHex 2.0: A Pattern-Based MCTS Hex Player. In International Conference on Computers and Games, pages 60–71. Springer, 2013.&nbsp;<a href="#fnref:MOHEX" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:OnlineOffline">
      <p>S. Gelly and D. Silver. Combining Online and Offline Knowledge in UCT. In Proceedings of the 24th International Conference on Machine learning, pages 273–280. ACM, 2007.&nbsp;<a href="#fnref:OnlineOffline" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  <span class="social-icons"><a href="https://www.facebook.com/sharer.php?u=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/"  class="social-icons" target="_blank" title="Share on facebook"> <i class="fa fa-facebook meta"></i></a> 
<a href="https://twitter.com/share?text=Thinking Fast and Slow with Deep Learning and Tree Search&amp;url=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/&amp;via=davidobarber"  class="social-icons" target="_blank" title="Share on twitter"> <i class="fa fa-twitter meta"></i></a>
<a href="https://plus.google.com/share?url=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/"  class="social-icons" target="_blank" title="Share on Google+"> <i class="fa fa-google-plus"></i></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/"  class="social-icons" target="_blank" title="Share on LinkedIn"> <i class="fa fa-linkedin"></i></a>
<a href="http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/#disqus_thread" class="social-icons" title="Comments"><i class="fa fa-comments"></i></a>
<!--<a href="http://localhost:4000/featured" title="Featured posts"><i class="fa fa-paperclip" title="Featured" class="social-icons" style="vertical-align: top;"></i></a>-->
<a href="javascript:window.print()" class="social-icons" title="Printer friendly format"><i class="fa fa-print"></i></a>
</span>

<!--

<div class="share-page">
    Share this on &rarr;
    <a href="https://facebook.com/sharer.php?u=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a>
</div>
-->
  <hr>
  
  <span class="post-date metafoot" itemprop="datePublished" content="2017-10-30"><i class="fa fa-calendar" title="Date published"> <a class="permalink" href="http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/" itemprop="url" title="Permanent link to this post">30 Oct 2017</a> </i></span>
  <span class="post-tags" itemprop="keywords" content="deep learning, Monte Carlo Tree Search, Hex, reinforcement learning, AlphaGo, and Dual Process Theory"><i class="fa fa-tags" title="page tags"></i> <a href="http://localhost:4000/tags/#deep+learning" title="Pages tagged deep learning" rel="tag">deep learning</a> &bull;  <a href="http://localhost:4000/tags/#Monte+Carlo+Tree+Search" title="Pages tagged Monte Carlo Tree Search" rel="tag">Monte Carlo Tree Search</a> &bull;  <a href="http://localhost:4000/tags/#Hex" title="Pages tagged Hex" rel="tag">Hex</a> &bull;  <a href="http://localhost:4000/tags/#reinforcement+learning" title="Pages tagged reinforcement learning" rel="tag">reinforcement learning</a> &bull;  <a href="http://localhost:4000/tags/#AlphaGo" title="Pages tagged AlphaGo" rel="tag">AlphaGo</a> &bull;  <a href="http://localhost:4000/tags/#Dual+Process+Theory" title="Pages tagged Dual Process Theory" rel="tag">Dual Process Theory</a></span>
    
</div>


  <div class="printMsg">
<table>
  <thead>
    <tr>
      <th><i class="fa fa-twitter">@davidobarber</i></th>
      <th>QR code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><i class="fa fa-anchor"> http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/</i><br /><i class="fa fa-calendar"> 30-Oct-17</i><br /><i class="fa fa-creative-commons"> BY-NC-SA 4.0 http://localhost:4000/disclosure</i></td>
      <td><img src="https://chart.googleapis.com/chart?chs=150x150&cht=qr&chl=http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/&choe=UTF-8" alt="http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/" /></td>
    </tr>
  </tbody>
</table>
</div>



<div class="page-break"></div>
<div class="related">
  <h2>Related Posts</h2>
<ul>
  
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/">Some modest insights into the error surface of Neural Nets</a><br /></li>
          
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/04/03/variational-optimisation/">Evolutionary Optimization as a Variational Method</a><br /></li>
          
    
  
     
       
        
          <li><a href="http://localhost:4000/blog/2017/03/15/large-number-of-classes/">Training with a large number of classes</a><br /></li>
          
    
  
</ul>
</div>

<div class="prevnext">
  
    <a class="prevnext-item older" href="http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/" title="Some modest insights into the error surface of Neural Nets">Older</a>
  
  
    <span class="prevnext-item older">Newer</span>
  
</div>

<div class="page-break"></div>

<div id="disqus_thread"></div><!-- /#disqus_thread -->



                    <div class="custom-footer" style="display: block;">
            <div class="footer-social-icons">
            <ul class="social-icons">
                      
            </ul>
            </div>
            </div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
    

<!-- gist embed -->

  <!--Gist embed -->
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.4/gist-embed.min.js"></script>



<!-- disqus comments -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'davidbarber'; 
    var disqus_identifier = 'http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


</body>
</html>
