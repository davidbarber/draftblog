<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>David Barber</title>
    <description>David's blog...</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <updated>2017-11-01T00:08:14+00:00</updated>
    <id>http://localhost:4000</id>
    <author>
      <name>David Barber</name>
    </author>
    
      <item>
        <title>Thinking Fast and Slow with Deep Learning and Tree Search</title>
        
          <description>&lt;p&gt;How to train powerful reinforcement learning agents by Thinking Fast and Slow.&lt;/p&gt;

&lt;!--more--&gt;


&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\sq}[1]{\left[#1\right]}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\ave}[1]{\mathbb{E}\sq{#1}}&lt;/script&gt;&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;dual-process-theory&quot;&gt;Dual Process Theory&lt;/h2&gt;

&lt;p&gt;According to &lt;a href=&quot;https://en.wikipedia.org/wiki/Dual_process_theory&quot;&gt;dual-process theory&lt;/a&gt; human reasoning consists of two different kinds of thinking.
System 1, is a fast, unconscious and automatic mode of thought, also known as intuition or heuristic process. System 2, an evolutionarily recent process unique to humans, is a slow, conscious, explicit and rule-based mode of reasoning.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//behaviour-design-predicting-irrational-decisions-12-638.jpg&quot; alt=&quot;dual process&quot; title=&quot;dual process theory&quot; /&gt;
&lt;a href=&quot;https://www.slideshare.net/AshDonaldson/behaviour-design-predicting-irrational-decisions&quot;&gt;image credit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When learning to complete a challenging planning task, such as playing a board game, humans exploit both processes: strong intuitions allow for more effective analytic reasoning by rapidly selecting interesting lines of play for consideration. Repeated deep study gradually improves intuitions. Stronger intuitions feedback to stronger analysis, creating a closed learning loop. In other words, humans learn by thinking fast and slow.&lt;/p&gt;

&lt;!--### What's wrong with Deep RL?
{:.no_toc}--&gt;

&lt;p&gt;In current Deep Reinforcement Learning algorithms such as Policy Gradients&lt;sup id=&quot;fnref:Williams&quot;&gt;&lt;a href=&quot;#fn:Williams&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and DQN&lt;sup id=&quot;fnref:DQN&quot;&gt;&lt;a href=&quot;#fn:DQN&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, neural networks make action selections with no lookahead; this is analogous to System 1. Unlike human intuition, their training does not benefit from a ‘System 2’ to suggest strong policies. Another issue is that state-of-the-art RL board playing algorithms require an initial database of human expert play&lt;sup id=&quot;fnref:AlphaGo&quot;&gt;&lt;a href=&quot;#fn:AlphaGo&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. Making a state-of-the-art board game player &lt;em&gt;ex nihilo&lt;/em&gt; is a major challenge for AI.&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;expert-iteration-exit&quot;&gt;Expert Iteration (ExIt)&lt;/h2&gt;

&lt;p&gt;In our &lt;a href=&quot;&quot;&gt;NIPSexitpaper&lt;/a&gt;, we introduced Expert Iteration (ExIt), which is a new and general framework for learning.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//ExIt.png&quot; alt=&quot;ExIt&quot; title=&quot;Expert Iteration&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ExIt can be viewed as an extension of Imitation Learning (IL) methods to domains where the best known experts are unable to achieve satisfactory performance. In stanadard IL an apprentice is trained to imitate the behaviour of an expert.  In ExIt, we extend this to an iterative learning process.  Between each iteration, we perform an Expert Improvement step, where we bootstrap the (fast) apprentice policy to increase the performance of the (comparatively slow) expert.&lt;/p&gt;

&lt;p&gt;To give some intuition around this idea, consider playing a board game such as chess. Here the expert is analogous to a strong (but slow) chess player, and the apprentice is analogous to an initially novice (but quick thinking) chess player.&lt;/p&gt;

&lt;p&gt;Initially the expert player plays some games against an opponent. The expert is a strong player, thinking deep (and slow) about each move. The apprentice observes the chess board state and each eventual move made by the expert and then tries his best to learn to quickly imitate the move made by the expert in each of the observed board positions . In an algorithm, this imitation could be done, for example, by fitting a neural network to the move made by the expert from a game position.  The apprentice learns a fast policy that is able to quickly imitate the play of the expert on the moves seen so far.  A key point here is that the apprentice learns in such a way that they are able to generalise and then apply their quick intution on positions not previously seen – the neural network thus plays the role of both generalising and imitating the play of the expert.&lt;/p&gt;

&lt;p&gt;Now that the apprentice has learned a fast imitation of the expert (on the moves seen so far), she can try to be of use to the expert.  One approach would be that when the expert now wishes to make a move, a small set of candidate moves are suggested very quickly by the apprentice which the expert can then consider in depth, possibly also guided during this slow thought process by other quick suggestions by the apprentice.&lt;/p&gt;

&lt;p&gt;At the end of this phase, the expert will have made a set of apprentice-aided moves, with each move being typically  much stronger than the apprentice could have made with the deep (but slow) thought of the expert.&lt;/p&gt;

&lt;p&gt;The above process now repeats, with the apprentice retraining on the moves suggested by the expert. This completes one full iteration of the learning phase of the apprentice and we iterate this process until we either run out of time or the apprentice learns to perform at roughly the same level as the expert.&lt;/p&gt;

&lt;p&gt;From a Dual Process perspective, the Imitation Learning step is analogous to a human improving their intuition for the task by studying example problems, while the Expert Improvement step is analogous to a human using the their improved intuition to guide future analysis&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;tree-search-and-deep-learning&quot;&gt;Tree Search and Deep Learning&lt;/h3&gt;

&lt;p&gt;Exit is a very general strategy for learning and the apprentice and expert can be specified in a variety of ways. In board games Monte Carlo Tree Search is a strong playing strategy&lt;sup id=&quot;fnref:MCTS&quot;&gt;&lt;a href=&quot;#fn:MCTS&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; and is a natural candidate to play the role of the expert.  Deep Learning has been shown to be a successfull method to imitate the play of strong players and it is therefore a natural candidate to play the role of the apprentice&lt;sup id=&quot;fnref:AlphaGo:1&quot;&gt;&lt;a href=&quot;#fn:AlphaGo&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The expert policy is calculated using a tree search algorithm. We use the apprentice to improve such an expert by using the apprentice policy to direct search effort towards promising moves, or by evaluating states encountered during search more quickly and accurately using our apprentice, effectively reducing the search breadth and depth. In other words, we bootstrap the knowledge acquired by Imitation Learning back into the planning algorithm.&lt;/p&gt;

&lt;!--The role of the expert is to perform exploration, and thereby to accurately determine strong move sequences, from a single position. The role of the apprentice is to generalise the policies that the expert discovers across the whole state space, and to provide rapid access to that strong policy for bootstrapping in future searches. The canonical choice of expert is a tree search algorithm. Search considers the exact dynamics of the game tree local to the state under consideration, and can be considered analogous to the lookahead human games players engage in when planning their moves. The bootstrap policy can be used to bias search towards promising moves, aid node evaluation, or both. By employing search, we can find promising sequences potentially far away from the bootstrap policy, accelerating learning in complex scenarios. Possible tree search algorithms include Monte Carlo Tree Search[^MCTS]--&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;the-boardgame-hex&quot;&gt;The Boardgame Hex&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Hex_(board_game)&quot;&gt;Hex&lt;/a&gt; is a classic two-player boardgame played on an &lt;script type=&quot;math/tex&quot;&gt;n\times n&lt;/script&gt; hexagonal grid. The players, denoted by colours black and white, alternate placing stones of their colour in empty cells. The black player wins if there is a sequence of adjacent black stones connecting the North edge of the board to the South edge. White wins if he achieves a sequence of adjacent white stones running from the West edge to the East edge.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//hexBW.png&quot; alt=&quot;hex&quot; title=&quot;Hex&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above represents play on a &lt;script type=&quot;math/tex&quot;&gt;5\times 5&lt;/script&gt; board, with white winning (reproduced from [^Hex]).   Hex has deep strategy, making it challenging for machines to play and its large action set and connection-based rules means it shares similar challenges for AI to Go. Compared to Go, however, the rules are simpler and there can be no draws, making it an ideal testbed for AI.&lt;/p&gt;

&lt;p&gt;Because the rules of Hex are so simple, the game is amenable to mathematical analysis and the current best machine player MOHEX&lt;sup id=&quot;fnref:MOHEX&quot;&gt;&lt;a href=&quot;#fn:MOHEX&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; uses a combination of Monte Carlo Tree Search and smart mathematical insights.  MOHEX has won every Computer Games Olympiad Hex tournament since 2009. MOHEX is also trained on datasets of human expert play.&lt;/p&gt;

&lt;p&gt;We wanted to see if we can outperform MOHEX by using our ExIt strategy, learning trained tabula rasa, without game-specific knowledge or human example play, beside the rules of the game. To do this, our expert is a MCTS player that is guided by the apprentice neural network.  Our neural network is based on a deep convolutional network, as shown below&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//NN.png&quot; alt=&quot;NN&quot; title=&quot;NN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The expert improvement step is then modelled by using the modified MCTS formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;UCT(s,a) + w \frac{\hat{\pi}(a|s)}{n(s,a)+1}&lt;/script&gt;

&lt;p&gt;This formula is closely related to one found in Gelly and Silver&lt;sup id=&quot;fnref:OnlineOffline&quot;&gt;&lt;a href=&quot;#fn:OnlineOffline&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. Here &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; is the state of the Hex board, &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is a possible action (ie move) from &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;. The term &lt;script type=&quot;math/tex&quot;&gt;UCT(s,a)&lt;/script&gt; represents the classical Upper Confidence Tree formula&lt;sup id=&quot;fnref:MCTS:1&quot;&gt;&lt;a href=&quot;#fn:MCTS&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; used in MCTS.&lt;/p&gt;

&lt;p&gt;The additional term denotes apprentice  helping guide the search to more promising moves, with &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}&lt;/script&gt; being the suggestion of the apprentice, and &lt;script type=&quot;math/tex&quot;&gt;n(s,a)&lt;/script&gt; the number of visits currently made by the search algorithm through state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and taking action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;; &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; is an empirically chosen weighting factor that balances the slow thinking of the expert with the fast intuition of the apprentice.&lt;/p&gt;

&lt;p&gt;To generate the data for traiing the apprentice (during each Imitation Learning phase), the batch approach generates data afresh, discarding all data from previous iterations. In the online version we consider instead a buffer of the most recent moves generted and we also consider an exponentially weighted version that favours more recent moves generated (since the more recent moves will be be from a stronger player).  A comparison of these different approaches is given below in which we compare the strength of the learned algorithm against a measure of training time.  We also show a more classical approach, known as REINFORCE (see &lt;a href=&quot;&quot;&gt;NIPSexitpaper&lt;/a&gt; for details)&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//BatchOnline.png&quot; alt=&quot;results&quot; title=&quot;results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What the figure shows is that the ExIt approach is considerably more effective than classical approaches.  Indeed, after training, our apprentice-aided MCTS player outperforms the best known machine Hex player, namely MOHEX, beating it in 71% of games played on a &lt;script type=&quot;math/tex&quot;&gt;9\times 9&lt;/script&gt; board.&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;relation-to-alphago-zero&quot;&gt;Relation to AlphaGo Zero&lt;/h3&gt;

&lt;p&gt;AlphaGo Zero [? ], published shortly after this work was first published, also implements the ExIt algorithm, and shows that it is able to achieve state-of-the-art performance in Go. Blah…..&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;ExIt is great…… Blah&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:Williams&quot;&gt;
      &lt;p&gt;R. J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Mach. Learn., 8(3-4):229–256, 1992.&amp;nbsp;&lt;a href=&quot;#fnref:Williams&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:DQN&quot;&gt;
      &lt;p&gt;V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-Level Control through Deep Reinforcement Learning. Nature, 518(7540):529–533, 2015.&amp;nbsp;&lt;a href=&quot;#fnref:DQN&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:AlphaGo&quot;&gt;
      &lt;p&gt;D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature, 529(7587):484–489, 2016.&amp;nbsp;&lt;a href=&quot;#fnref:AlphaGo&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:AlphaGo:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:MCTS&quot;&gt;
      &lt;p&gt;L. Kocsis and C. Szepesvári. Bandit Based Monte-Carlo Planning. In European Conference on Machine Learning, pages 282–293. Springer, 2006.&amp;nbsp;&lt;a href=&quot;#fnref:MCTS&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:MCTS:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:MOHEX&quot;&gt;
      &lt;p&gt;S.-C. Huang, B. Arneson, R. Hayward, M. Müller, and J. Pawlewicz. MoHex 2.0: A Pattern-Based MCTS Hex Player. In International Conference on Computers and Games, pages 60–71. Springer, 2013.&amp;nbsp;&lt;a href=&quot;#fnref:MOHEX&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:OnlineOffline&quot;&gt;
      &lt;p&gt;S. Gelly and D. Silver. Combining Online and Offline Knowledge in UCT. In Proceedings of the 24th International Conference on Machine learning, pages 273–280. ACM, 2007.&amp;nbsp;&lt;a href=&quot;#fnref:OnlineOffline&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        
        <pubDate>Mon, 30 Oct 2017 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/10/30/Thinking-Fast-and-Slow-with-Deep-Learning-and-Tree-Search/</guid>
      </item>
    
      <item>
        <title>Some modest insights into the error surface of Neural Nets</title>
        
          <description>&lt;p&gt;Did you know that feedforward Neural Nets (with piecewise linear transfer functions) have no smooth local maxima?&lt;/p&gt;

&lt;p&gt;In our recent ICML paper &lt;a href=&quot;http://proceedings.mlr.press/v70/botev17a.html&quot;&gt;Practical Gauss-Newton Optimisation for Deep Learning&lt;/a&gt;) we discuss a second order method that can be applied successfully to accelerate training of Neural Networks. However, here I want to discuss some of the fairly straightforward, but perhaps interesting, insights into the geometry of the error surface that that work gives.&lt;/p&gt;

&lt;!--more--&gt;


&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\br}[1]{\left(#1\right)}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\sq}[1]{\left[#1\right]}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\ave}[1]{\mathbb{E}\sq{#1}}&lt;/script&gt;&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;feedforward-neural-networks&quot;&gt;Feedforward Neural Networks&lt;/h2&gt;

&lt;p&gt;In our description, a feedforward NN takes an input vector &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and produces a vector &lt;script type=&quot;math/tex&quot;&gt;h_L&lt;/script&gt; on the final &lt;script type=&quot;math/tex&quot;&gt;L^{th}&lt;/script&gt; layer. We write &lt;script type=&quot;math/tex&quot;&gt;h_\lambda&lt;/script&gt; to be the vector of pre-activation values for layer &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;a_\lambda&lt;/script&gt; to denote the vector of activation values after passing through the transfer function &lt;script type=&quot;math/tex&quot;&gt;f_\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Starting with setting &lt;script type=&quot;math/tex&quot;&gt;a_0&lt;/script&gt;  to the input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, a feedforward NN is defined by the recursion&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\lambda = W_\lambda a_{\lambda-1}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;W_\lambda&lt;/script&gt; is the weight matrix of layer &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; (we use a sub or superscript &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; wherever most convenient) and the activation vector is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_\lambda = f_\lambda(h_\lambda)&lt;/script&gt;

&lt;p&gt;We define a loss &lt;script type=&quot;math/tex&quot;&gt;E(h_L,y)&lt;/script&gt; between the final output layer &lt;script type=&quot;math/tex&quot;&gt;h_L&lt;/script&gt; and a desired training output &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;. For example, we might use a squared loss&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(h_L,y) = (h_L-y)^2&lt;/script&gt;

&lt;p&gt;where the loss is summed over all elements of the vector.  For a training dataset the total error function is the summed  loss over individual training points&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{E}(\theta) = \sum_{n=1}^N E(h_L(n),y(n))&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; represents the stacked vector of all parameters of the network. For simplicity we will write &lt;script type=&quot;math/tex&quot;&gt;E(\theta)&lt;/script&gt; for the error for a single generic datapoint.&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;the-gradient&quot;&gt;The Gradient&lt;/h3&gt;

&lt;p&gt;For training a NN, a key quantity is the gradient of the error&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_{i} = \frac{\partial}{\partial\theta_i} E(\theta)&lt;/script&gt;

&lt;p&gt;We use this for example in gradient descent training algorithms. An important issue is how to compute the gradient efficiently. Thanks to the layered structure of the network, it’s intuitive that there is an efficient scheme (backprop which is a special case of Reverse Mode AutoDiff) that propagates information from layer to layer.&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;the-hessian&quot;&gt;The Hessian&lt;/h3&gt;

&lt;p&gt;One aspect of the structure of the error surface is the local curvature, defined by the Hessian matrix with elements&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{ij} = \frac{\partial^2}{\partial\theta_i\partial\theta_j} E(\theta)&lt;/script&gt;

&lt;p&gt;The Hessian matrix itself is typically very large. To make this more manageable, we’ll focus here on the Hessian of the parameters of a given layer &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.  That is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[H_\lambda]_{(a,b),(c,d)} = \frac{\partial^2 E}{\partial W^\lambda_{a,b}\partial W^\lambda_{c,d}}&lt;/script&gt;

&lt;p&gt;The Hessians &lt;script type=&quot;math/tex&quot;&gt;H_\lambda&lt;/script&gt; then form the diagonal block matrices of the full Hessian &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;.&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;a-recursion-for-the-hessian&quot;&gt;A recursion for the Hessian&lt;/h2&gt;

&lt;p&gt;Similar to the gradient, it’s perhaps intuitive that a recursion exists to calculate this layerwise Hessian.  Starting from&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E}{\partial W^\lambda_{a,b}}=\sum_i \frac{\partial h^\lambda_i}{W^\lambda_{a,b}}\frac{\partial E}{\partial h^\lambda_i} = a^{\lambda-1}_b\frac{\partial E}{\partial h^\lambda_a}&lt;/script&gt;

&lt;p&gt;and differentiating again we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[H_\lambda]_{(a,b),(c,d)} = a^{\lambda-1}_b a^{\lambda-1}_d [{\cal{H}}_\lambda]_{a,c}
\tag{1}\label{eq:H}&lt;/script&gt;

&lt;p&gt;where we define the pre-activation Hessian for layer &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[{\cal{H}}_\lambda]_{a,c} = \frac{\partial^2 E}{\partial h^\lambda_a\partial h^\lambda_c}&lt;/script&gt;

&lt;p&gt;We show in &lt;a href=&quot;http://proceedings.mlr.press/v70/botev17a.html&quot;&gt;Practical Gauss-Newton Optimisation for Deep Learning&lt;/a&gt; that one can derive a simple backwards recursion for this pre-activation Hessian (the recursion is for a single datapoint – the total Hessian &lt;script type=&quot;math/tex&quot;&gt;\bar{H}&lt;/script&gt; is a sum over the individual datapoint Hessians):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\cal{H}}_\lambda = B_\lambda W_{\lambda+1}^\top {\cal{H}}_{\lambda+1}W_{\lambda+1}B_{\lambda}+D_\lambda
\tag{2}\label{eq:recursion}&lt;/script&gt;

&lt;p&gt;where we define the diagonal matrices&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B_\lambda = \text{diag}(f'_\lambda(h_\lambda))&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_\lambda = \text{diag}\br{f''_\lambda(h_\lambda)\frac{\partial E}{\partial a_\lambda}}&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;f'&lt;/script&gt; is the first derivative of the transfer function and &lt;script type=&quot;math/tex&quot;&gt;f''&lt;/script&gt; is the second derivative.&lt;/p&gt;

&lt;p&gt;The recursion is initialised with &lt;script type=&quot;math/tex&quot;&gt;{\cal{H}}_L&lt;/script&gt; which depends on the objective &lt;script type=&quot;math/tex&quot;&gt;E(h_L,y)&lt;/script&gt; and is easily calculated for the usual loss functions. For example, for the square loss &lt;script type=&quot;math/tex&quot;&gt;(y-h_L)^2/2&lt;/script&gt; we have &lt;script type=&quot;math/tex&quot;&gt;{\cal{H}}_L=I&lt;/script&gt;, namely the identity matrix. We use this recursion in our &lt;a href=&quot;http://proceedings.mlr.press/v70/botev17a.html&quot;&gt;paper&lt;/a&gt; to build an approximate Gauss-Newton optimisation method.&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;consequences&quot;&gt;Consequences&lt;/h2&gt;

&lt;p&gt;Piecewise linear transfer functions, such as the ReLU &lt;script type=&quot;math/tex&quot;&gt;f(x) = \max(x,0)&lt;/script&gt; are currently popular due to both their speed of evaluation (compared to more traditional transfer functions such as &lt;script type=&quot;math/tex&quot;&gt;\tanh(x)&lt;/script&gt;) and also the empirical observation that, under gradient based training, they tend to get trapped less often in local optima. Note that if the transfer functions are piecewise linear, this does not necessarily mean that the objective will be piecewise linear (since the loss is usually itself not piecewise linear).&lt;/p&gt;

&lt;p&gt;For a piecewise linear transfer function, apart from the `nodes’ where the linear sections meet, the function is differentiable and has zero second derivative, &lt;script type=&quot;math/tex&quot;&gt;f''(x)=0&lt;/script&gt;. This means that the matrices &lt;script type=&quot;math/tex&quot;&gt;D_\lambda&lt;/script&gt; in the above Hessian recursion will be zero (away from nodes).&lt;/p&gt;

&lt;p&gt;For many common loss functions, such as squared loss (for regression) and cross entropy loss (for classification) the Hessian &lt;script type=&quot;math/tex&quot;&gt;{\cal{H}}_L&lt;/script&gt; is Positive Semi-Definite (PSD).&lt;/p&gt;

&lt;p&gt;Note that, according to \eqref{eq:recursion}, for transfer functions that contain zero gradient points &lt;script type=&quot;math/tex&quot;&gt;f'(x)=0&lt;/script&gt; then the Hessian &lt;script type=&quot;math/tex&quot;&gt;H_\lambda&lt;/script&gt; can have lower rank than &lt;script type=&quot;math/tex&quot;&gt;H_{\lambda+1}&lt;/script&gt;, reducing the curvature information propagating back from layers close to the output towards layers closer to the input. This has the effect of creating flat plateaus in the surface and makes gradient based training potentially more problematic. Conversely, provided the gradient of the transfer function is never zero &lt;script type=&quot;math/tex&quot;&gt;f'\neq 0&lt;/script&gt;, then according to \eqref{eq:recursion} each layer pre-activation Hessian is Positive Definite, helping preserve the propagation of surface curvature back through the network.&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;structure-within-a-layer&quot;&gt;Structure within a layer&lt;/h3&gt;

&lt;p&gt;For such loss functions, it follows that the pre-activation Hessian &lt;script type=&quot;math/tex&quot;&gt;{\cal{H}}_\lambda&lt;/script&gt; for all layers is PSD as well (away from nodes).  It immediately follows from \eqref{eq:H} that the Hessian &lt;script type=&quot;math/tex&quot;&gt;H_\lambda&lt;/script&gt; for each layer &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is PSD.  This means that, if we fix all the parameters of the network, and vary only  the parameters in a layer &lt;script type=&quot;math/tex&quot;&gt;W^\lambda&lt;/script&gt;, then the objective &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt; can exhibit no smooth local maxima or smooth saddle points.  Note that this does not imply that the objective is convex everywhere with respect to &lt;script type=&quot;math/tex&quot;&gt;W_\lambda&lt;/script&gt; as the surface will contain ridges corresponding to the non-differentiable nodes.&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;no-differentiable-local-maxima&quot;&gt;No differentiable local maxima&lt;/h3&gt;

&lt;p&gt;The trace of the full Hessian &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; is the sum of the traces of each of the layerwise blocks &lt;script type=&quot;math/tex&quot;&gt;H_\lambda&lt;/script&gt;. Since (as usual away from nodes) by the above argument each matrix &lt;script type=&quot;math/tex&quot;&gt;H_\lambda&lt;/script&gt; is PSD, it follows that the trace of the full Hessian is non-negative.  This means that it is not possible for all eigenvalues of the Hessian to be simultaneously negative, with the immediate consequence that feedforward networks (with piecewise linear transfer functions) have no differentiable local maxima. The picture below illustrates the kind of situtation therefore that can happen in terms of local maxima:&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//blogpost_canhappen.png&quot; alt=&quot;blogpost_canhappen&quot; title=&quot;can happen&quot; /&gt;&lt;/p&gt;

&lt;p&gt;whereas the image below depicts the kind of smooth local maxima that cannot happen:&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//blogpost_canthappen.png&quot; alt=&quot;blogpost_canthappen&quot; title=&quot;cannot happen&quot; /&gt;&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;visualisation-for-a-simple-two-layer-net&quot;&gt;Visualisation for a simple two layer net&lt;/h3&gt;

&lt;p&gt;We consider a simple network with two layers, ReLU transfer functions and square loss error. The network thus has two weight matrices &lt;script type=&quot;math/tex&quot;&gt;W^1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W^2&lt;/script&gt;.  Below we choose two fixed matrices &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; and parameterise the weight matrix &lt;script type=&quot;math/tex&quot;&gt;W^1&lt;/script&gt; as a function of two scalars &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;, so that &lt;script type=&quot;math/tex&quot;&gt;W^1(u,v)=uU + vV&lt;/script&gt;.  As we vary &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; we then plot the objective function &lt;script type=&quot;math/tex&quot;&gt;E(u,v)&lt;/script&gt;, keeping all other parameters of the network fixed.&lt;/p&gt;

&lt;p&gt;As we can see the surface contains no local differentiable local maxima as we vary the parameters in the layer.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//rectlinE1.png&quot; alt=&quot;rectlinE1&quot; title=&quot;rectlin E1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below we show an analogous plot for varying the parameters of the second layer weights &lt;script type=&quot;math/tex&quot;&gt;W^2(u,v)&lt;/script&gt;, which has the same predicted property that there are no differentiable local maxima.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//rectlinE2.png&quot; alt=&quot;rectlinE2&quot; title=&quot;rectlin E2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, below we plot &lt;script type=&quot;math/tex&quot;&gt;E(u,v)&lt;/script&gt; using &lt;script type=&quot;math/tex&quot;&gt;W^1=uU&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W^2=vV&lt;/script&gt;, showing how the objective function changes as we simultaneously change the parameters in different layers. As we can see, there are no differentiable maxima.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//rectlinE12.png&quot; alt=&quot;rectlinE12&quot; title=&quot;rectlin E12&quot; /&gt;&lt;/p&gt;

&lt;h1 class=&quot;no_toc&quot; id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;A simple consequence of using piecewise linear transfer functions and a convex loss, is that feedforward networks cannot have any differentiable maxima (or saddle points) as parameters are varied within a layer. Furthermore, the objective cannot contain any differentiable maxima, even as we vary parameters across layers. Note that the objective &lt;script type=&quot;math/tex&quot;&gt;E(u,v)&lt;/script&gt; though can (and empirically does) have smooth saddle points as one varies parameters &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; across &lt;em&gt;different&lt;/em&gt; layers.&lt;/p&gt;

&lt;p&gt;It’s unclear how practically significant these modest insights are. However, they do potentially partially support the use of piecewise linear transfer functions (particularly those with no zero gradient regions) since for such transfer functions  gradient based training algorithms cannot easily dawdle on local maxima (anywhere), or idle around saddle points (within a layer) since such regions correspond to sharp slopes in the objective.&lt;/p&gt;

&lt;p&gt;These results are part of a more detailed study of second order methods for optimisation in feedforward Neural Nets which will appear in &lt;a href=&quot;http://proceedings.mlr.press/v70/botev17a.html&quot;&gt;ICML 2017&lt;/a&gt;.&lt;/p&gt;

&lt;!--
{:.no_toc}
--&gt;

</description>
        
        <pubDate>Sun, 30 Jul 2017 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/07/30/Optima-in-Feedforward-Neural-Nets/</guid>
      </item>
    
      <item>
        <title>Evolutionary Optimization as a Variational Method</title>
        
          <description>&lt;p&gt;A simple connection between evolutionary optimisation and variational methods.&lt;/p&gt;

&lt;!--more--&gt;


&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\sq}[1]{\left[#1\right]}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\ave}[1]{\mathbb{E}\sq{#1}}&lt;/script&gt;&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;variational-optimization&quot;&gt;Variational Optimization&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1212.4507&quot;&gt;Variational Optimization&lt;/a&gt; is based on the bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min_x f(x) \leq \ave{f(x)}_{p(x|\theta)}&lt;/script&gt;

&lt;p&gt;That is, the minimum of a collection of values is always less than their average.  By defining&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U(\theta) = \ave{f(x)}_{p(x|\theta)}&lt;/script&gt;

&lt;p&gt;instead of minimising &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; with respect to &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, we can minimise the upper bound &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; with respect to &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Provided the distribution &lt;script type=&quot;math/tex&quot;&gt;p(x\vert \theta)&lt;/script&gt; is rich enough, this will be equivalent to minimising &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The gradient of the upper bound is then given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial U}{\partial \theta} = \ave{f(x)\frac{\partial}{\partial \theta}\log p(x|\theta)}_{p(x|\theta)}&lt;/script&gt;

&lt;p&gt;which is reminiscent of the REINFORCE (Williams 1992) policy gradient approach in Reinforcement Learning.&lt;/p&gt;

&lt;p&gt;In  the original VO &lt;a href=&quot;https://arxiv.org/abs/1212.4507&quot;&gt;report&lt;/a&gt;  and &lt;a href=&quot;https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-65.pdf&quot;&gt;paper&lt;/a&gt; this idea was used to form a differentiable upper bound for non-differentiable &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and also discrete &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.&lt;/p&gt;

&lt;h3 class=&quot;no_toc&quot; id=&quot;sampling-approximation&quot;&gt;Sampling Approximation&lt;/h3&gt;

&lt;p&gt;There is an interesting connection to evolutionary computation (more precisely &lt;a href=&quot;https://arxiv.org/abs/1212.4507&quot;&gt;Estimation of Distribution Algorithms&lt;/a&gt;) if the expectation with respect to &lt;script type=&quot;math/tex&quot;&gt;p(x\vert \theta)&lt;/script&gt; is performed using sampling. In this case one can draw samples &lt;script type=&quot;math/tex&quot;&gt;x^1,\ldots,x^S&lt;/script&gt; from &lt;script type=&quot;math/tex&quot;&gt;p(x\vert\theta)&lt;/script&gt; and form an unbiased approximation to the upper bound gradient&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial U}{\partial \theta} \approx \frac{1}{S} \sum_{s}f(x^s)\frac{\partial}{\partial \theta}\log p(x^s|\theta)&lt;/script&gt;

&lt;p&gt;The “evolutionary” connection is that the samples &lt;script type=&quot;math/tex&quot;&gt;x^s&lt;/script&gt; can be thought of as “particles” or “swarm members” that are used to estimate the gradient. Based on the approximate gradient, simple Stochastic Gradient Descent (SGD) would then perform the parameter update (for learning rate &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^{new} = \theta-\frac{\eta}{S} \sum_{s}f(x^s)\frac{\partial}{\partial \theta}\log p(x^s|\theta)&lt;/script&gt;

&lt;p&gt;The “swarm” then disperses and draws a new set of members from &lt;script type=&quot;math/tex&quot;&gt;p(x\vert \theta^{new})&lt;/script&gt; and the process repeats.&lt;/p&gt;

&lt;p&gt;A special case of VO is to use a Gaussian so that (for the scalar case – the multivariate setting follows similarly)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U(\theta) = \frac{1}{\sqrt{2\pi\sigma^2}}\int e^{-\frac{1}{2\sigma^2}(x-\theta)^2}f(x)dx&lt;/script&gt;

&lt;p&gt;Then the gradient of this upper bound is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U'(\theta) = \frac{1}{\sigma^2}\ave{(x-\theta)f(x)}_{x\sim N(\theta,\sigma^2)}&lt;/script&gt;

&lt;p&gt;By changing variable &lt;script type=&quot;math/tex&quot;&gt;\epsilon=x-\theta&lt;/script&gt; this is equivalent to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U'(\theta) = \frac{1}{\sigma^2}\ave{\epsilon f(\theta+\epsilon)}_{\epsilon \sim N(0,\sigma^2)}
\label{eq:grad}\tag{1}&lt;/script&gt;

&lt;p&gt;Fixing &lt;script type=&quot;math/tex&quot;&gt;\sigma=5&lt;/script&gt; and using &lt;script type=&quot;math/tex&quot;&gt;S=10&lt;/script&gt; samples, we show below the trajectory (for 150 steps of SGD with fixed learning rate &lt;script type=&quot;math/tex&quot;&gt;\eta=0.1&lt;/script&gt;) of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; based on Stochastic VO and compare this to the underlying function &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt; (which in this case is a simple quadratic).  Note that we only plot below the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; trajectory (each red dot represents a parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, with the initial parameter in the bottom right) and not the samples from &lt;script type=&quot;math/tex&quot;&gt;p(x\vert \theta)&lt;/script&gt;.  As we see, despite the noisy gradient estimate, the parameter values &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; move toward the minimum of the objective &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;.  The matlab code is &lt;a href=&quot;https://gist.github.com/davidbarber/16708b9135f13c9599f754f4010a0284&quot;&gt;available&lt;/a&gt; if you’d like to play with this.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//VO2Dss5.png&quot; alt=&quot;fixing sigma5&quot; title=&quot;fixed sigma 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One can also consider the bound as a function of both the mean &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U(\theta,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\int e^{-\frac{1}{2\sigma^2}(x-\theta)^2}f(x)dx&lt;/script&gt;

&lt;p&gt;and minimise the bound with respect to both &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; (which we will parameterise using &lt;script type=&quot;math/tex&quot;&gt;\sigma^2=e^\beta&lt;/script&gt; to ensure a positive variance). More generally, one can consider parameterising the Gaussian covariance matrix for example using factor analysis and minimsing the bound with respect to the factor loadings.&lt;/p&gt;

&lt;p&gt;Using a Gaussian with covariance &lt;script type=&quot;math/tex&quot;&gt;e^\beta I&lt;/script&gt; and performing gradient descent on both &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, for the same objective function, learning rate &lt;script type=&quot;math/tex&quot;&gt;\eta=0.1&lt;/script&gt;  and initial &lt;script type=&quot;math/tex&quot;&gt;\sigma=5&lt;/script&gt;, we obtain the trajectory below for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//Vo2Dssgrad.png&quot; alt=&quot;learning sigma5&quot; title=&quot;learned sigma 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, by learning &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;, the trajectory is much less noisy and more quickly homes in on the optimum.  The trajectory of the learned standard deviation &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; is given below, showing how the variance reduces as we home in on the optimum.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//VO2Dsdtraj.png&quot; alt=&quot;learning sigma traj 5&quot; title=&quot;learned sigma traj 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the context of more general optimisation problems (such as in deep learning and reinforcement learning), VO is potentially interesting since the sampling process can be distributed across different machines.&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;gradient-approximation-by-gaussian-perturbation&quot;&gt;Gradient Approximation by Gaussian Perturbation&lt;/h2&gt;

&lt;!--
which is the same as equation $(\ref{eq:grad})$ above on interchanging $x$ with $\theta$.  A simple optimisation strategy is then gradient descent

$$
\theta^{new} = \theta - \eta U'(\theta)
$$

where $U'(\theta)$ can be approximated by sampling. This would then be fully equivalent to the approach suggested in [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/abs/1703.03864). 


This shows that the &quot;evolutionary approach&quot; is in fact a special case of VO (using an isotropic Gaussian). A potential benefit of this insight is that the upper bound gives a principled way to adjust parameters, such as not just the mean $\theta$ but also the variance $\sigma^2$. 




## Approximating the Gradient by Sampling
{:.no_toc}
--&gt;

&lt;p&gt;Ferenc Huszar‏ has a nice post &lt;a href=&quot;http://www.inference.vc/evolutionary-strategies-embarrassingly-parallelizable-optimization/&quot;&gt;Evolution Strategies: Almost Embarrassingly Parallel Optimization&lt;/a&gt; summarising recent work by Salimans etal on &lt;a href=&quot;https://arxiv.org/abs/1703.03864&quot;&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The aim is to minimise a function &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt; by using gradient based approaches, without explicitly calculating the gradient. The first observation is that the gradient can be approximated by considering the Taylor expansion&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x+\epsilon) = f(x)+\epsilon f'(x) + \frac{\epsilon^2}{2} f''(x) + O(\epsilon^3)&lt;/script&gt;

&lt;p&gt;Multiplying both sides by &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon f(x+\epsilon) = \epsilon f(x)+\epsilon^2 f'(x) +\frac{\epsilon^3}{2}f''(x)+ O(\epsilon^4)&lt;/script&gt;

&lt;p&gt;Finally, taking the expectation with respect to &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; drawn from a Gaussian distribution with zero mean and variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ave{\epsilon f(x+\epsilon)} = \sigma^2 f'(x) + O(\epsilon^4)&lt;/script&gt;

&lt;p&gt;Hence, we have the approximation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f'(x) \approx \frac{1}{\sigma^2}\ave{\epsilon f(x+\epsilon)}
\label{eq:grad2}\tag{2}&lt;/script&gt;

&lt;p&gt;Based on the above discussion of VO, and comparing equations (1) and (2) we see that this Gaussian perturbation approach is related to VO in which we use a Gaussian &lt;script type=&quot;math/tex&quot;&gt;p(x\vert \theta)&lt;/script&gt;, with the understanding that in the VO case the optimisation is over &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; rather than &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.  An advantage of the VO approach, however, is that it provides a principled way to adjust parameters such as the variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; (based on minimising the upper bound).&lt;/p&gt;

&lt;p&gt;Whilst this was derived for the scalar setting, the vector derivative is obtained by applying the same method, where the &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; vector is drawn from the zero mean multivariate Gaussian with covariance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2 I&lt;/script&gt; for identity matrix &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt;.&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;efficient-communication&quot;&gt;Efficient Communication&lt;/h2&gt;

&lt;p&gt;A key insight in &lt;a href=&quot;https://arxiv.org/abs/1703.03864&quot;&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&lt;/a&gt; is that the sampling process can be distributed across multiple machines, &lt;script type=&quot;math/tex&quot;&gt;i\in\{1,\ldots,S\}&lt;/script&gt; so that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f'(x) \approx \frac{1}{S\sigma^2}\sum_{i=1}^S {\epsilon^i f(x+\epsilon^i)}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\epsilon^i&lt;/script&gt; is a vector sample and &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is the sample index. Each machine &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; can then calculate &lt;script type=&quot;math/tex&quot;&gt;f(x+\epsilon^i)&lt;/script&gt;. The Stochastic Gradient parameter update with learning rate &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{new} = x - \frac{\eta}{S\sigma^2}\sum_{i=1}^S {\epsilon^i f(x+\epsilon^i)}&lt;/script&gt;

&lt;p&gt;Provided each machine &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; also knows the random seed used to generate the &lt;script type=&quot;math/tex&quot;&gt;\epsilon^j&lt;/script&gt; of each other machine, it therefore knows what all the &lt;script type=&quot;math/tex&quot;&gt;\epsilon^j&lt;/script&gt; are (by sampling according to the known seeds) and can thus calculate &lt;script type=&quot;math/tex&quot;&gt;x^{new}&lt;/script&gt; based on only the &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; scalar values calculated by each machine. The basic point here is that, thanks to seed sharing, there is no requirement to send the vectors &lt;script type=&quot;math/tex&quot;&gt;\epsilon^i&lt;/script&gt; between the machines (only the scalar values &lt;script type=&quot;math/tex&quot;&gt;f(x+\epsilon^i)&lt;/script&gt; need be sent), keeping the transmission costs very low.&lt;/p&gt;

&lt;p&gt;Based on the insight that the &lt;a href=&quot;https://arxiv.org/abs/1703.03864&quot;&gt;Parallel Gaussian Perturbation&lt;/a&gt; approach is a special case of VO, it would be natural to apply VO using seed sharing to efficiently parallelise the sampling. This has the benefit that other parameters such as the variance can also be efficiently communicated, potentially significantly speeding up convergence.&lt;/p&gt;

&lt;!--
One can view this as an &quot;evolutionary&quot; optimisation approach in which a collection of particles $\epsilon^1,\ldots,\epsilon^S$ is created at each iteration of Stochastic Gradient Descent.


where $U'(\theta)$ can be approximated by sampling. This would then be fully equivalent to the approach suggested in [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/abs/1703.03864). 


This shows that the &quot;evolutionary approach&quot; is in fact a special case of VO (using an isotropic Gaussian). A potential benefit of this insight is that the upper bound gives a principled way to adjust parameters, such as not just the mean $\theta$ but also the variance $\sigma^2$. 
--&gt;

</description>
        
        <pubDate>Mon, 03 Apr 2017 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/2017/04/03/variational-optimisation/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/04/03/variational-optimisation/</guid>
      </item>
    
      <item>
        <title>Training with a large number of classes</title>
        
          <description>&lt;p&gt;In machine learning we often face the issue of a very large number of classes in a classification problem. This causes a bottleneck in the computation. There’s though a simple and effective way to deal with this.&lt;/p&gt;

&lt;!--more--&gt;


&lt;h2 class=&quot;no_toc&quot; id=&quot;probabilistic-classification&quot;&gt;Probabilistic Classification&lt;/h2&gt;
&lt;p&gt;In areas like Natural Language Processing (NLP) a common task is to predict the next word in sequence (like in preditictive text on a smartphone or in learning word embeddings).  For input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and class label &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt;, the probability of predicting class &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_\theta(c|x) = \frac{u_\theta(c,x)}{Z_\theta(x)}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;u_\theta(c,x)&lt;/script&gt; is some defined function with parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. For example, &lt;script type=&quot;math/tex&quot;&gt;u_\theta(c,x)=\exp(w_c'x)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;w_c&lt;/script&gt; is a parameter vector for class &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the vector input.  The normalising term is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_\theta(x) = \sum_{c=1}^C u_\theta(c,x)&lt;/script&gt;

&lt;p&gt;The task is then to adjust the parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; to maximise the probability of the correct class for each of the training points.&lt;/p&gt;

&lt;p&gt;However, if there are &lt;script type=&quot;math/tex&quot;&gt;C=100,000&lt;/script&gt; words in the dictionary, this means calculating the normalisation &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; for each datapoint is going to be expensive.  There have been a variety of approaches suggested over the years to make computationally efficient approximations, many based on importance sampling.&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;why-plain-importance-sampling-doesnt-work&quot;&gt;Why plain Importance Sampling doesn’t work&lt;/h2&gt;

&lt;p&gt;A standard approach to approximating &lt;script type=&quot;math/tex&quot;&gt;Z_\theta(x)&lt;/script&gt; is to use&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_\theta(x) = \sum_{c=1}^C q(c) \frac{u_\theta(c,x)}{q(c)}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is an importance distribution over all &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; classes.  We can then form an approximation by sampling from &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; a small number &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; of classes to form a sample bag &lt;script type=&quot;math/tex&quot;&gt;{\cal{S}}&lt;/script&gt; and using&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_\theta(x) \approx \tilde{Z}_\theta(x) = \frac{1}{S}\sum_{s\in{\cal{S}}}  \frac{u_\theta(s,x)}{q(s)}&lt;/script&gt;

&lt;p&gt;The problem with this approach is that it results in a potentially catastrophic under-estimate of &lt;script type=&quot;math/tex&quot;&gt;Z_\theta(x)&lt;/script&gt;.  If the classifier is working well, we want that &lt;script type=&quot;math/tex&quot;&gt;u_\theta(c,x)&lt;/script&gt; is much higher than &lt;script type=&quot;math/tex&quot;&gt;u_\theta(d,x)&lt;/script&gt; for any incorrect class &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;.  Hence, unless the importance sample bag &lt;script type=&quot;math/tex&quot;&gt;{\cal{S}}&lt;/script&gt; includes class &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt;, then the normalisation approximation will miss this significant mass and the probability approximation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{u_\theta(c,x)}{\tilde{Z}_\theta(x)}&lt;/script&gt;

&lt;p&gt;will be wildly inaccurate, see figure (a) below.  This is the source of the historically well-documented instabilities in training large-scale classifiers.&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;making-importance-sampling-work&quot;&gt;Making Importance Sampling work&lt;/h2&gt;

&lt;p&gt;However, there is an easy fix for this – simply ensure that &lt;script type=&quot;math/tex&quot;&gt;{\cal{S}}&lt;/script&gt; includes the correct class &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt;.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//aistats17.png&quot; alt=&quot;fixing IS&quot; title=&quot;fixing IS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the left above we show for &lt;script type=&quot;math/tex&quot;&gt;C=10,000&lt;/script&gt; classes the ratio &lt;script type=&quot;math/tex&quot;&gt;u_\theta(c,x)/Z_\theta(x)&lt;/script&gt; on the &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-axis against its approximation  &lt;script type=&quot;math/tex&quot;&gt;u_\theta(c,x)/\tilde{Z}_\theta(x)&lt;/script&gt; on the &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;-axis. Each dot represents a different randomly drawn set of &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; values. Red, green and blue represent 10, 20 and 50 importance samples respectively. The ideal estimation would be such that all points are along the line &lt;script type=&quot;math/tex&quot;&gt;y=x&lt;/script&gt;.  Note the vertical scale – these values are supposed to be probabilities and lie between 0 and 1.  Even as we increase the number of importance samples, this remains a wildly incorrect estimation of the probability.&lt;/p&gt;

&lt;p&gt;On the right above we show the same probability estimate but now simply also include the correct class in the set &lt;script type=&quot;math/tex&quot;&gt;{\cal{S}}&lt;/script&gt;. The vertical scale is now sensible and the estimated probabiliy is close to the true value.&lt;/p&gt;

&lt;h2 class=&quot;no_toc&quot; id=&quot;deep-learning-recurrent-nlp-models&quot;&gt;Deep Learning Recurrent NLP models&lt;/h2&gt;

&lt;p&gt;We applied this method to learning word embeddings for a deep
recurrent network.  The training objective was standard maximum
likelihood, but with the normalisation approximation above. Below we
plot the exact log likelihood (&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;-axis) against the optimisation
gradient ascent iteration (&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;-axis). We also plot the exact log
likelihood for some alternative training approaches. As we
see, standard Importance Sampling becomes unstable as learning
progresses. However our simple modification stabilizes learning and is
competitive against a range of alternatives including Noise
Contrastive Estimation, Ranking approaches, Negative Sampling and
BlackOut.&lt;/p&gt;

&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/images//aistats17_2.png&quot; alt=&quot;fixing IS&quot; title=&quot;fixing IS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is so simple and works so well that we use this in all our NLP deep learning training experiments.&lt;/p&gt;

&lt;p&gt;This forms the basis for our paper &lt;a href=&quot;http://web4.cs.ucl.ac.uk/staff/D.Barber/publications/AISTATS2017.pdf&quot;&gt;Complementary Sum Sampling for Likelihood Approximation in Large Scale Classification&lt;/a&gt; which will appear in &lt;a href=&quot;http://www.aistats.org/&quot;&gt;AISTATS 2017&lt;/a&gt;.&lt;/p&gt;
</description>
        
        <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/blog/2017/03/15/large-number-of-classes/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/03/15/large-number-of-classes/</guid>
      </item>
    
  </channel>
</rss>
